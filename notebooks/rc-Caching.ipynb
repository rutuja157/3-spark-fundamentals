{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5bd5e2-b28a-49e4-a313-d22ee339b4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://b79bc3db1652:4040\n",
       "SparkContext available as 'sc' (version = 3.5.5, master = local[*], app id = local-1753464973647)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "<console>",
     "evalue": "2: error: ';' expected but '#' found.",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: ';' expected but '#' found.",
      "       #change the kernal to spylon-kernel and then run",
      "       ^",
      "<console>:8: error: Invalid literal number",
      "       #we dont want to do at event level , we wanna aggregate to the userid and device id level 1st in eventsAggregated and go from there",
      "                                                                                                 ^",
      ""
     ]
    }
   ],
   "source": [
    "#change the kernal to spylon-kernel and then run\n",
    "#what this pipeline is doing is-this spark code is doing is-> we have list of users who have bunch of users\n",
    "#but we also have bunch of devices\n",
    "#users can have many devices but device can also have many users- many to many both sides\n",
    "#so we are doing here is create both sides users - and all there devices and also a  device - and all there users doing that \n",
    "#so one of the thing we do initially is we also want the we doing that with eventsAggregated cfeate it and use it \n",
    "#we dont want to do at event level , we wanna aggregate to the userid and device id level 1st in eventsAggregated and go from there\n",
    "#then take eventsAggregated and join in back to itself and then kinda aggregate up in userAndDevices and in devicesOnEvents\n",
    "#This pipeline is trivial because we are reusing eventsAggregated dataframe in this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea161da-68fa-4a2e-971a-0ad1380d6d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://b79bc3db1652:4040\n",
       "SparkContext available as 'sc' (version = 3.5.5, master = local[*], app id = local-1753641519536)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.storage.StorageLevel\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.storage.StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7226cb85-8eb2-4b4e-9504-dd75f2ba9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1b0bc-4427-4186-8633-c3eb0a89cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast join threshold: Disables auto-broadcasting joins.\n",
    "# Fail ambiguous self join: Allows self joins without errors.\n",
    "# Shuffle partitions: Limits the number of shuffle partitions (for joins, groupBy, etc.) to 4 for better performance on small data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40358403-e81c-42bd-9eb8-97faaaaa1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two types of caching\n",
    "# .cache() or .persist(StorageLevel.MEMORY_ONLY) which will store data inmemory and can resue it in spark \n",
    "# .persist(storageLevel.Disk_ONLY) this is not much usefullunless you createtableand save this to that table likebelow\n",
    "#eventsAggregated.write.mode(\"overwrite\").saveAsTable(\"bootcamp.events_aggregated_staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e1ae61-e7ba-416b-a689-3005d67227fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: int, device_id: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/events.csv\")\n",
    "                        .where($\"user_id\".isNotNull)\n",
    "\n",
    "users.createOrReplaceTempView(\"events\") \n",
    "# In PySpark: Just replace val with regular variables and drop the $ for column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd69397-f0cb-48a9-b616-9eb98a2d6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#users.createOrReplaceTempView(\"events\") -->users dataframe into sql temp table \"events\" can use events in sql query only for current session\n",
    "#// Using SQL to query your DataFrame\n",
    "# val highActivity = spark.sql(\"\"\"\n",
    "#   SELECT user_id, COUNT(*) as cnt\n",
    "#   FROM events\n",
    "#   GROUP BY user_id\n",
    "#   HAVING cnt > 10\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7096b569-6dbf-4bd0-a862-eb3db20294c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+--------------------+---+--------------------+\n",
      "|    user_id|device_id|referrer|                host|url|          event_time|\n",
      "+-----------+---------+--------+--------------------+---+--------------------+\n",
      "| 1037710827|532630305|    NULL| www.zachwilson.tech|  /|2021-03-08 17:27:...|\n",
      "|  925588856|532630305|    NULL|    www.eczachly.com|  /|2021-05-10 11:26:...|\n",
      "|-1180485268|532630305|    NULL|admin.zachwilson....|  /|2021-02-17 16:19:...|\n",
      "|-1044833855|532630305|    NULL| www.zachwilson.tech|  /|2021-09-24 15:53:...|\n",
      "|  747494706|532630305|    NULL| www.zachwilson.tech|  /|2021-09-26 16:03:...|\n",
      "+-----------+---------+--------+--------------------+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee0ba52-a6ea-4ba8-bbef-92d2dc267485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+--------------------+---+--------------------+\n",
      "|    user_id|device_id|referrer|                host|url|          event_time|\n",
      "+-----------+---------+--------+--------------------+---+--------------------+\n",
      "| 1037710827|532630305|    NULL| www.zachwilson.tech|  /|2021-03-08 17:27:...|\n",
      "|  925588856|532630305|    NULL|    www.eczachly.com|  /|2021-05-10 11:26:...|\n",
      "|-1180485268|532630305|    NULL|admin.zachwilson....|  /|2021-02-17 16:19:...|\n",
      "|-1044833855|532630305|    NULL| www.zachwilson.tech|  /|2021-09-24 15:53:...|\n",
      "|  747494706|532630305|    NULL| www.zachwilson.tech|  /|2021-09-26 16:03:...|\n",
      "|  747494706|532630305|    NULL|admin.zachwilson....|  /|2021-02-21 16:08:...|\n",
      "| -824540328|532630305|    NULL|admin.zachwilson....|  /|2021-09-28 17:23:...|\n",
      "| -824540328|532630305|    NULL|    www.eczachly.com|  /|2021-09-29 01:22:...|\n",
      "| 1833036683|532630305|    NULL|admin.zachwilson....|  /|2021-01-24 03:15:...|\n",
      "|-2134824313|532630305|    NULL|    www.eczachly.com|  /|2021-01-25 00:03:...|\n",
      "+-----------+---------+--------+--------------------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM events LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f76f63b8-1588-4a36-a5b5-31b44b24157b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "highActivity: org.apache.spark.sql.DataFrame = [user_id: int, cnt: bigint]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Using SQL to query your DataFrame\n",
    "val highActivity = spark.sql(\"\"\"\n",
    "  SELECT user_id, COUNT(*) as cnt\n",
    "  FROM events\n",
    "  GROUP BY user_id\n",
    "  HAVING cnt > 10\n",
    "\"\"\")\n",
    "# This query is running on your DataFrame, not an actual database table! You have created a DataFrame called highActivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "debcc0a7-8572-4793-96d0-da562a321ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|    user_id|cnt|\n",
      "+-----------+---+\n",
      "| -173749422| 16|\n",
      "| 1399665425| 16|\n",
      "|  632739597| 16|\n",
      "| 1072106763| 33|\n",
      "|  210988258| 32|\n",
      "|   18650769| 25|\n",
      "|-1072136053| 20|\n",
      "|  203689086| 21|\n",
      "| 1231244260| 13|\n",
      "|-1563553498| 24|\n",
      "+-----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "highActivity.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad29fb23-f730-4f15-affd-ade8bbd02016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- referrer: string (nullable = true)\n",
      " |-- host: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8d2e16-6f46-4dce-8c09-02d8b07cd52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "devices: org.apache.spark.sql.DataFrame = [device_id: int, browser_type: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val devices = spark.read.option(\"header\", \"true\")\n",
    "                       .option(\"inferSchema\", \"true\")\n",
    "                       .csv(\"/home/iceberg/data/devices.csv\")\n",
    "\n",
    "devices.createOrReplaceTempView(\"devices\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "332cefba-6be5-486e-a926-91d715587184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------+------------------+\n",
      "|  device_id|        browser_type|os_type|       device_type|\n",
      "+-----------+--------------------+-------+------------------+\n",
      "|-2147042689|             Firefox| Ubuntu|             Other|\n",
      "|-2146219609|            WhatsApp|  Other|            Spider|\n",
      "|-2145574618|       Chrome Mobile|Android|Generic Smartphone|\n",
      "|-2144707350|Chrome Mobile Web...|Android|  Samsung SM-G988B|\n",
      "|-2143813999|Mobile Safari UI/...|    iOS|            iPhone|\n",
      "+-----------+--------------------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "devices.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e4c5643-cf5d-48dc-92ff-4d0410f4ad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- browser_type: string (nullable = true)\n",
      " |-- os_type: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "devices.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5e92b8c-61e1-470b-8fb2-d1c4ca7e30d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eventsAggregated: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: int, device_id: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val eventsAggregated = spark.sql(f\"\"\"\n",
    "  SELECT user_id, \n",
    "         device_id, \n",
    "         COUNT(1) as event_counts, \n",
    "         COLLECT_LIST(DISTINCT host) as host_array\n",
    "  FROM events\n",
    "  GROUP BY 1,2\n",
    "\"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb9b1fd8-5424-4bf4-94f3-41f8997b3aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+--------------------+\n",
      "|    user_id|  device_id|event_counts|          host_array|\n",
      "+-----------+-----------+------------+--------------------+\n",
      "|-2147421007| -807271869|           1|[admin.zachwilson...|\n",
      "|-2147340867| 1324700293|           1|  [www.eczachly.com]|\n",
      "|-2147051672|  583904608|           1|  [www.eczachly.com]|\n",
      "|-2146961540| 1436415199|           1|   [dataengineer.io]|\n",
      "|-2146752203| -887516106|           1|  [www.eczachly.com]|\n",
      "|-2146501484| -240714045|           1|  [www.eczachly.com]|\n",
      "|-2146204366|-1893334816|           1|  [www.eczachly.com]|\n",
      "|-2146151971| 1177446421|           5|  [www.eczachly.com]|\n",
      "|-2146090945|-1238295066|           1|  [www.eczachly.com]|\n",
      "|-2146031264| 1439772194|           1|  [www.eczachly.com]|\n",
      "+-----------+-----------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- event_counts: long (nullable = false)\n",
      " |-- host_array: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res8: Long = 141935\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eventsAggregated.show(10)\n",
    "eventsAggregated.printSchema()\n",
    "eventsAggregated.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44873b55-466d-4159-bca3-90b6e106770b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.DataFrame = []\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bootcamp.events_aggregated_staging (\n",
    "        user_id BIGINT,\n",
    "        device_id BIGINT,\n",
    "        event_counts BIGINT,\n",
    "        host_array ARRAY<STRING>\n",
    "    )\n",
    "    PARTITIONED BY (ds STRING)\n",
    "\"\"\")\n",
    "\n",
    "# This just creates a table definition in your metastore/catalog.\n",
    "# Data is NOT written yet (your save command is commented out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1c5ccc9-8df3-4833-8c3c-554580bb3e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usersAndDevices: org.apache.spark.sql.DataFrame = [user_id: int, user_id: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val usersAndDevices = users\n",
    "  .join(eventsAggregated, eventsAggregated(\"user_id\") === users(\"user_id\"))\n",
    "  .groupBy(users(\"user_id\"))\n",
    "  .agg(\n",
    "    users(\"user_id\"),\n",
    "    max(eventsAggregated(\"event_counts\")).as(\"total_hits\"),\n",
    "    collect_list(eventsAggregated(\"device_id\")).as(\"devices\")\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196289e5-968a-47da-b99c-837c53bdf754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins users and aggregated events by user_id.\n",
    "# Groups by user, finds max event count (per user), and collects device_ids per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d44964-945b-4b5a-97f9-51f62be0a5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+----------------------------------------------+\n",
      "|user_id    |user_id    |total_hits|devices                                       |\n",
      "+-----------+-----------+----------+----------------------------------------------+\n",
      "|-2147470439|-2147470439|3         |[378988111, 378988111, 378988111]             |\n",
      "|-2147421007|-2147421007|1         |[-807271869]                                  |\n",
      "|-2147326548|-2147326548|1         |[532630305]                                   |\n",
      "|-2147051672|-2147051672|1         |[1237528510, 1237528510, 583904608, 583904608]|\n",
      "|-2146931906|-2146931906|1         |[532630305]                                   |\n",
      "|-2146892655|-2146892655|2         |[-1516742733, -1516742733]                    |\n",
      "|-2146848244|-2146848244|1         |[891045274]                                   |\n",
      "|-2146743833|-2146743833|1         |[-258252617]                                  |\n",
      "|-2146598128|-2146598128|1         |[-723516686]                                  |\n",
      "|-2146535052|-2146535052|2         |[1142619498, 1142619498]                      |\n",
      "+-----------+-----------+----------+----------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- total_hits: long (nullable = true)\n",
      " |-- devices: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersAndDevices.show(10, truncate=false)\n",
    "usersAndDevices.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9171e51a-0c7b-42d6-a287-2ad982c61722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "devicesOnEvents: org.apache.spark.sql.DataFrame = [device_id: int, device_type: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val devicesOnEvents = devices\n",
    "  .join(eventsAggregated, devices(\"device_id\") === eventsAggregated(\"device_id\"))\n",
    "  .groupBy(devices(\"device_id\"), devices(\"device_type\"))\n",
    "  .agg(\n",
    "    devices(\"device_id\"),\n",
    "    devices(\"device_type\"),\n",
    "    collect_list(eventsAggregated(\"user_id\")).as(\"users\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3e6ed-2c9a-4cb4-bd13-8c3e28e7a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins devices with aggregated events on device_id.\n",
    "# Groups by device & type, collects all user_ids who used the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d19f7155-033b-4a9e-8ff7-5df76cb02f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+----------------+---------------------------------------------------+\n",
      "|device_id  |device_type     |device_id  |device_type     |users                                              |\n",
      "+-----------+----------------+-----------+----------------+---------------------------------------------------+\n",
      "|-2147042689|Other           |-2147042689|Other           |[1633522354]                                       |\n",
      "|-2146219609|Spider          |-2146219609|Spider          |[-2119549067]                                      |\n",
      "|-2144707350|Samsung SM-G988B|-2144707350|Samsung SM-G988B|[332947552]                                        |\n",
      "|-2143813999|iPhone          |-2143813999|iPhone          |[-945726225]                                       |\n",
      "|-2142634982|iPhone          |-2142634982|iPhone          |[47777302]                                         |\n",
      "|-2142350383|iPhone          |-2142350383|iPhone          |[-1744927308, 1798466603, -1431608087, -1395457020]|\n",
      "|-2141256237|iPhone          |-2141256237|iPhone          |[2130663607]                                       |\n",
      "|-2138977887|Other           |-2138977887|Other           |[1519288440]                                       |\n",
      "|-2136667425|Other           |-2136667425|Other           |[126477390]                                        |\n",
      "|-2136444196|iPhone          |-2136444196|iPhone          |[-1235274712, 385018576]                           |\n",
      "+-----------+----------------+-----------+----------------+---------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- users: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "devicesOnEvents.show(10, truncate=false)\n",
    "devicesOnEvents.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434a9b3-aee4-406d-9d9c-dd010968df51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd54ea7-a560-4929-9122-ec4e44ffee8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a57880-db6c-4bf9-a258-4cca66cbd7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38620b-e197-4fed-9ede-8d3c2952e043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ae4c8b-4599-4fbb-a545-76b6e3bcb84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ObjectHashAggregate(keys=[device_id#598, device_type#601], functions=[collect_list(user_id#568, 0, 0)])\n",
      "   +- ObjectHashAggregate(keys=[device_id#598, device_type#601], functions=[partial_collect_list(user_id#568, 0, 0)])\n",
      "      +- Project [device_id#598, device_type#601, user_id#568]\n",
      "         +- SortMergeJoin [device_id#598], [device_id#569], Inner\n",
      "            :- Sort [device_id#598 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(device_id#598, 4), ENSURE_REQUIREMENTS, [plan_id=735]\n",
      "            :     +- Filter isnotnull(device_id#598)\n",
      "            :        +- FileScan csv [device_id#598,device_type#601] Batched: false, DataFilters: [isnotnull(device_id#598)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/devices.csv], PartitionFilters: [], PushedFilters: [IsNotNull(device_id)], ReadSchema: struct<device_id:int,device_type:string>\n",
      "            +- Sort [device_id#569 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(device_id#569, 4), ENSURE_REQUIREMENTS, [plan_id=736]\n",
      "                  +- Filter isnotnull(device_id#569)\n",
      "                     +- InMemoryTableScan [user_id#568, device_id#569], [isnotnull(device_id#569)]\n",
      "                           +- InMemoryRelation [user_id#568, device_id#569, event_counts#606L, host_array#607], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                 +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                                    +- ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[count(1), collect_list(distinct host#20, 0, 0)])\n",
      "                                       +- Exchange hashpartitioning(user_id#17, device_id#18, 4), ENSURE_REQUIREMENTS, [plan_id=752]\n",
      "                                          +- ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[merge_count(1), partial_collect_list(distinct host#20, 0, 0)])\n",
      "                                             +- HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[merge_count(1)])\n",
      "                                                +- Exchange hashpartitioning(user_id#17, device_id#18, host#20, 4), ENSURE_REQUIREMENTS, [plan_id=748]\n",
      "                                                   +- HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[partial_count(1)])\n",
      "                                                      +- Filter isnotnull(user_id#17)\n",
      "                                                         +- FileScan csv [user_id#17,device_id#18,host#20] Batched: false, DataFilters: [isnotnull(user_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/events.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:int,device_id:int,host:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ObjectHashAggregate(keys=[user_id#568], functions=[max(event_counts#606L), collect_list(device_id#569, 0, 0)])\n",
      "   +- ObjectHashAggregate(keys=[user_id#568], functions=[partial_max(event_counts#606L), partial_collect_list(device_id#569, 0, 0)])\n",
      "      +- Project [user_id#568, device_id#569, event_counts#606L]\n",
      "         +- SortMergeJoin [user_id#568], [user_id#614], Inner\n",
      "            :- Sort [user_id#568 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(user_id#568, 4), ENSURE_REQUIREMENTS, [plan_id=788]\n",
      "            :     +- Filter isnotnull(user_id#568)\n",
      "            :        +- FileScan csv [user_id#568,device_id#569] Batched: false, DataFilters: [isnotnull(user_id#568)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/events.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:int,device_id:int>\n",
      "            +- Sort [user_id#614 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(user_id#614, 4), ENSURE_REQUIREMENTS, [plan_id=789]\n",
      "                  +- Filter isnotnull(user_id#614)\n",
      "                     +- InMemoryTableScan [user_id#614, event_counts#606L], [isnotnull(user_id#614)]\n",
      "                           +- InMemoryRelation [user_id#614, device_id#615, event_counts#606L, host_array#607], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                 +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                                    +- ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[count(1), collect_list(distinct host#20, 0, 0)])\n",
      "                                       +- Exchange hashpartitioning(user_id#17, device_id#18, 4), ENSURE_REQUIREMENTS, [plan_id=805]\n",
      "                                          +- ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[merge_count(1), partial_collect_list(distinct host#20, 0, 0)])\n",
      "                                             +- HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[merge_count(1)])\n",
      "                                                +- Exchange hashpartitioning(user_id#17, device_id#18, host#20, 4), ENSURE_REQUIREMENTS, [plan_id=801]\n",
      "                                                   +- HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[partial_count(1)])\n",
      "                                                      +- Filter isnotnull(user_id#17)\n",
      "                                                         +- FileScan csv [user_id#17,device_id#18,host#20] Batched: false, DataFilters: [isnotnull(user_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/events.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:int,device_id:int,host:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.storage.StorageLevel\n",
       "users: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: int, device_id: int ... 4 more fields]\n",
       "devices: org.apache.spark.sql.DataFrame = [device_id: int, browser_type: string ... 2 more fields]\n",
       "executionDate: String = 2023-01-01\n",
       "eventsAggregated: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: int, device_id: int ... 2 more fields]\n",
       "usersAndDevices: org.apache.spark.sql.DataFrame = [user_id: int, user_id: int ... 2 more fields]\n",
       "devicesOnEvents: org.apache.spark.sql.DataFrame = [device_id: int, device_type: string ... 3 more fields]\n",
       "res1: Array[org.apache.spark.sql.Row] = Array([-2147470439,-2147470439,3,WrappedArray(378988111, 378988111, 378988111)])\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "\n",
    "val users = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/events.csv\")\n",
    "                        .where($\"user_id\".isNotNull)\n",
    "\n",
    "users.createOrReplaceTempView(\"events\")\n",
    "\n",
    "val devices = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/devices.csv\")\n",
    "\n",
    "devices.createOrReplaceTempView(\"devices\")\n",
    "\n",
    "val executionDate = \"2023-01-01\"\n",
    "\n",
    "# //Caching here should be < 5 GBs or used for broadcast join\n",
    "# //You need to tune executor memory otherwise it'll spill to disk and be slow\n",
    "# //Donreally try using any of the other StorageLevel besides MEMORY_ONLY\n",
    "\n",
    "val eventsAggregated = spark.sql(f\"\"\"\n",
    "  SELECT user_id, \n",
    "          device_id, \n",
    "        COUNT(1) as event_counts, \n",
    "        COLLECT_LIST(DISTINCT host) as host_array\n",
    "  FROM events\n",
    "  GROUP BY 1,2\n",
    "\"\"\").cache()\n",
    "\n",
    "// eventsAggregated.write.mode(\"overwrite\").saveAsTable(\"bootcamp.events_aggregated_staging\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bootcamp.events_aggregated_staging (\n",
    "        user_id BIGINT,\n",
    "        device_id BIGINT,\n",
    "        event_counts BIGINT,\n",
    "        host_array ARRAY<STRING>\n",
    "    )\n",
    "    PARTITIONED BY (ds STRING)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "val usersAndDevices = users\n",
    "  .join(eventsAggregated, eventsAggregated(\"user_id\") === users(\"user_id\"))\n",
    "  .groupBy(users(\"user_id\"))\n",
    "  .agg(\n",
    "    users(\"user_id\"),\n",
    "    max(eventsAggregated(\"event_counts\")).as(\"total_hits\"),\n",
    "    collect_list(eventsAggregated(\"device_id\")).as(\"devices\")\n",
    "  )\n",
    "\n",
    "val devicesOnEvents = devices\n",
    "      .join(eventsAggregated, devices(\"device_id\") === eventsAggregated(\"device_id\"))\n",
    "      .groupBy(devices(\"device_id\"), devices(\"device_type\"))\n",
    "      .agg(\n",
    "        devices(\"device_id\"),\n",
    "        devices(\"device_type\"),\n",
    "         collect_list(eventsAggregated(\"user_id\")).as(\"users\")\n",
    "      )\n",
    "\n",
    "devicesOnEvents.explain()\n",
    "usersAndDevices.explain()\n",
    "\n",
    "devicesOnEvents.take(1)\n",
    "usersAndDevices.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c36feefd-a8cd-4749-81fd-8f7883349ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: eventsAggregated.type = [user_id: int, device_id: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eventsAggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02886a49-4dc5-4e00-9cad-20926166f388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
